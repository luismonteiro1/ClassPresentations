<!DOCTYPE html>
<html lang="en"><head>
<script src="05_NeuralNetworks_files/libs/clipboard/clipboard.min.js"></script>
<script src="05_NeuralNetworks_files/libs/quarto-html/tabby.min.js"></script>
<script src="05_NeuralNetworks_files/libs/quarto-html/popper.min.js"></script>
<script src="05_NeuralNetworks_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="05_NeuralNetworks_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="05_NeuralNetworks_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="05_NeuralNetworks_files/libs/quarto-html/quarto-syntax-highlighting-dark-b651517ce65839d647a86e2780455cfb.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.24">

  <meta name="author" content="Author: Carlos Grilo | Collaboration: Gustavo Reis, Catarina Silva, Pedro Gago, Luís Monteiro">
  <title>Neural Networks</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="05_NeuralNetworks_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="05_NeuralNetworks_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="05_NeuralNetworks_files/libs/revealjs/dist/theme/quarto-327cab17de08e4a17faa7d61f48ae591.css">
  <link rel="stylesheet" href="custom.css">
  <link href="05_NeuralNetworks_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="05_NeuralNetworks_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="05_NeuralNetworks_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="05_NeuralNetworks_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="05_NeuralNetworks_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="05_NeuralNetworks_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-dark">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Neural Networks</h1>
  <p class="subtitle">Artificial Intelligence Applied to Games</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Author: Carlos Grilo | Collaboration: Gustavo Reis, Catarina Silva, Pedro Gago, Luís Monteiro 
</div>
</div>
</div>

</section>
<section>
<section id="the-brain" class="title-slide slide level1 center">
<h1>The Brain</h1>

</section>
<section id="the-brain-1" class="slide level2 smaller">
<h2>The brain</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Neurons are the fundamental unit of the nervous system tissue.</li>
</ul>
<div class="fragment">
<ul>
<li>A biological neuron has three types of components that are specially interesting for the understanding of artificial neurons:
<ul>
<li><strong>Dendrites</strong></li>
<li><strong>Soma</strong> (cell body)</li>
<li><strong>Axon</strong></li>
</ul></li>
</ul>
</div>
</div><div class="column" style="width:50%;">
<div class="fragment">
<div class="quarto-figure quarto-figure-center-right">
<figure>
<p><img data-src="Brain.png" class="quarto-figure quarto-figure-center-right" style="width:100.0%"></p>
</figure>
</div>
</div>
</div></div>
</section>
<section id="neuron-structure" class="slide level2 nonincremental">
<h2>Neuron Structure</h2>
<p>Each neuron is composed of a cellular body with several branches called <strong>dendrites</strong> and an isolated longer branch, called <strong>axon</strong>.</p>
<div class="fragment">
<ul>
<li><strong>Dendrites</strong> connect to other neurons’ <strong>axons</strong> through junctions called <strong>synapses</strong></li>
<li>A <strong>neuron</strong> can be connected to hundreds of thousands of other neurons</li>
</ul>
</div>
</section>
<section id="neuron-signal-propagation" class="slide level2">
<h2>Neuron Signal Propagation</h2>
<ul>
<li class="fragment"><p>Signals propagate between neurons through a complicated electrochemical reaction which leads <strong>synapses</strong> to produce chemical substances that enter through <strong>dendrites</strong>. This can raise or diminish the electrical potential of the cellular body.</p></li>
<li class="fragment"><p>If the electrical potential overpasses some limit, an electrical impulse is sent to the <strong>axon</strong>, which spreads by its ramifications, thus transmitting electrical signals to other neurons.</p></li>
</ul>
</section>
<section id="the-brain-2" class="slide level2 smaller">
<h2>The brain</h2>
<p>An average brain has something on the order of <em>100 billion neurons</em>. Each neuron is connected to up to <em>10,000 other neurons</em>, which means that the number of synapses is between <em>100 trillion and 1,000 trillion</em>.</p>
<div class="fragment">
<p>It has been observed that:</p>
<ul>
<li><strong>Often used connections become stronger</strong></li>
<li>Neurons sometimes <strong>form new connections with other neurons</strong></li>
<li>These mechanisms are thought to lead to <strong>learning</strong></li>
<li>Computation is strongly <strong>parallel</strong> and <strong>asynchronous</strong></li>
</ul>
</div>
</section></section>
<section>
<section id="artificial-neural-networks" class="title-slide slide level1 center">
<h1>Artificial Neural Networks</h1>

</section>
<section id="what-is-an-artificial-neural-network" class="slide level2">
<h2>What is an Artificial Neural Network?</h2>
<ul>
<li class="fragment">It is an information processing system with some characteristics in common with biological neural networks.</li>
<li class="fragment">It consists of an <strong>interconnected set of simple processing units</strong> called <strong><em>neurons</em></strong> whose functionality is vaguely similar to that of a biological neuron.</li>
<li class="fragment"><strong><em>Neurons</em></strong> communicate between them by sending signals through a large number of connections and information processing occurs as if it occurred simultaneously in several neurons.</li>
</ul>
</section>
<section id="artificial-neural-network-structure" class="slide level2">
<h2>Artificial Neural Network Structure</h2>
<div class="fragment">
<ul>
<li>Each neuron has:
<ul>
<li>A set of <strong>input connections</strong></li>
<li>A set of <strong>output connections</strong></li>
</ul></li>
</ul>
</div>
<ul>
<li class="fragment">Some neurons are connected to the “outside world” (some for inputs, others for output).</li>
<li class="fragment">Each connection has a <strong><em>weight</em></strong> associated to it. <strong>Weights are the main means of information storage of a network</strong>.</li>
</ul>
</section>
<section id="network-example" class="slide level2">
<h2>Network Example</h2>

<img data-src="ExampleNeuralNetwork.png" class="quarto-figure quarto-figure-center-right r-stretch quarto-figure-center" style="width:100.0%"></section>
<section id="network-example-1" class="slide level2 smaller">
<h2>Network Example</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Key components:</p>
<ul>
<li class="fragment"><strong>Input layer</strong>: Receives external inputs</li>
<li class="fragment"><strong>Hidden layers</strong>: Internal processing layers</li>
<li class="fragment"><strong>Output layer</strong>: Produces the network output</li>
<li class="fragment"><strong>Weights</strong>: Values on connections between neurons</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center-right">
<figure>
<p><img data-src="ExampleNeuralNetwork.png" class="quarto-figure quarto-figure-center-right" style="width:100.0%"></p>
</figure>
</div>
</div></div>
</section>
<section id="using-neural-networks" class="slide level2">
<h2>Using Neural Networks</h2>
<div class="fragment">
<p>In order to use a neural network, we have to decide:</p>
<ol type="1">
<li><strong>How many neurons</strong> to use and of <strong>what type are they</strong></li>
<li><strong>How will they be connected</strong></li>
</ol>
</div>
<div class="fragment">
<p>Then:</p>
<ol start="3" type="1">
<li>Initialize the <strong>weights</strong></li>
<li>Start the <strong>training phase</strong>, during which the weights are modified until the network works as desired (in some networks other things may be changed besides the weights)</li>
</ol>
</div>
</section>
<section id="simplified-model-of-a-neuron" class="slide level2">
<h2>Simplified Model of a Neuron</h2>
<h3 id="computation-process">Computation Process</h3>
<p>The computation in each neuron is simple: it receives signals from the input connections and computes a new activation value which is sent through its output connections.</p>
<ul>
<li class="fragment">The computation of this value is done in two phases:</li>
</ul>
<div class="fragment">
<ol type="1">
<li>First, a <strong>weighted sum of the inputs</strong> is computed</li>
<li>Then, the neuron’s <strong>activation value</strong> <span class="math inline">\(a\)</span> is computed using a so-called <strong>activation function</strong> which uses as input the value computed in the first phase</li>
</ol>
</div>
</section>
<section id="mathematical-model" class="slide level2">
<h2>Mathematical Model</h2>
<p><span class="math display">\[\text{in} = \sum w_i x_i - \theta\]</span></p>
<p><span class="math display">\[a = g(\text{in})\]</span></p>
<p>Where:</p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><span class="math inline">\(x_i\)</span> are the inputs</li>
<li><span class="math inline">\(w_i\)</span> are the weights</li>
<li><span class="math inline">\(\theta\)</span> is the <strong>bias</strong> (or offset)</li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li><span class="math inline">\(g\)</span> is the activation function</li>
<li><span class="math inline">\(a\)</span> is the output activation</li>
</ul>
</div></div>
</section>
<section id="mathematical-model-1" class="slide level2">
<h2>Mathematical Model</h2>

<img data-src="ModelNeuron.png" class="quarto-figure quarto-figure-center r-stretch" style="width:100.0%"></section>
<section id="the-bias-term" class="slide level2">
<h2>The Bias Term</h2>
<p>As we can see in the previous figure, there is a special input with value -1, with a weight called <strong><em>offset</em></strong> or <strong><em>bias</em></strong> represented as <strong><span class="math inline">\(\theta\)</span></strong>.</p>
<p>The role of <strong><span class="math inline">\(\theta\)</span></strong>’s value will be illustrated in the next slides.</p>
</section>
<section id="common-activation-functions" class="slide level2 smaller">
<h2>Common Activation Functions</h2>

<img data-src="ActivationFunctions.png" class="quarto-figure quarto-figure-center-right r-stretch quarto-figure-center" style="width:100.0%"><ol type="a">
<li class="fragment"><strong>Step function</strong>: Binary output (0 or 1)</li>
<li class="fragment"><strong>Sign function</strong>: Binary output (-1 or +1)</li>
<li class="fragment"><strong>Sigmoid function</strong>: Continuous output between 0 and 1</li>
</ol>
</section>
<section id="neuron-example" class="slide level2 smaller">
<h2>Neuron Example</h2>
<p>Let us consider a neuron with two inputs and with the step activation function. So:</p>
<p><span class="math display">\[a = \begin{cases}
1, &amp; \text{if } x_1 w_1 + x_2 w_2 - \theta &gt; 0 \\
0, &amp; \text{if } x_1 w_1 + x_2 w_2 - \theta &lt; 0
\end{cases}\]</span></p>
<p>This means that the neuron’s activation value (output) will be 1 if the weighted sum of the inputs is larger than <strong><span class="math inline">\(\theta\)</span></strong> and 0, otherwise.</p>
<p>Therefore, <strong><span class="math inline">\(\theta\)</span></strong> represents the <strong>minimum value</strong> that the weighted sum must have so that the neuron’s activation value can be 1.</p>
<p><strong><span class="math inline">\(\theta\)</span></strong> can be used with activation functions other than the step function.</p>
</section>
<section id="limitations-of-a-single-neuron" class="slide level2">
<h2>Limitations of a Single Neuron</h2>
<p>One neuron divides in two parts the inputs space with the straight line <span class="math inline">\(x_1 w_1 + x_2 w_2 - \theta = 0\)</span></p>

<img data-src="LimitationOfSingleNeuron.png" class="quarto-figure quarto-figure-center r-stretch" style="width:100.0%"></section>
<section id="vector-notation" class="slide level2">
<h2>Vector Notation</h2>
<p>It is common to use vectors notation to represent the sets of inputs and weights.</p>
<p>Using this notation, we can say:</p>
<p><span class="math display">\[a = \begin{cases}
1, &amp; \text{if   } \mathbf{X}^T \mathbf{W} &gt; \theta \\
0, &amp; \text{if   } \mathbf{X}^T \mathbf{W} \leq \theta
\end{cases}\]</span></p>
<p>where <strong><span class="math inline">\(\mathbf{X}\)</span></strong> represents the inputs vector and <strong><span class="math inline">\(\mathbf{W}\)</span></strong> the weights vector.</p>
</section>
<section id="hyperplanes" class="slide level2">
<h2>Hyperplanes</h2>
<ul>
<li class="fragment">In two dimensions (two inputs) the division is done by a <strong>straight line</strong></li>
<li class="fragment">In three dimensions, by a <strong>plane</strong></li>
</ul>
<div class="fragment">
<ul>
<li><p>In larger dimensional spaces, we say that the division is done by a <strong>hyperplane</strong></p></li>
<li><p>This implies that a <strong>single neuron is only able to represent <em>linearly separable functions</em></strong>, which constitutes a serious limitation on the number of functions that can be represented.</p></li>
</ul>
</div>
</section>
<section id="example-logic-functions" class="slide level2">
<h2>Example: Logic Functions</h2>
<p>For example, it is very easy to build a network with just one element that behaves as a logic <strong><em>AND</em></strong> or a logic <strong><em>OR</em></strong>, but it is impossible to represent a function as simple as the logic <strong><em>XOR</em></strong>:</p>

<img data-src="VectorNotation.png" class="quarto-figure quarto-figure-center r-stretch" style="width:100.0%"></section></section>
<section>
<section id="neural-networks-topologies" class="title-slide slide level1 center">
<h1>Neural Networks Topologies</h1>

</section>
<section id="neural-networks-topologies-1" class="slide level2">
<h2>Neural Networks Topologies</h2>
<p>There are different types of network topologies, each one with its own characteristics. The main distinction is between:</p>
<h3 id="feed-forward-networks">Feed-forward Networks</h3>
<p>Where the connections are unidirectional and there are no feedback loops.</p>
<h3 id="recurrent-networks">Recurrent Networks</h3>
<p>Where there are feedback loops (for example, the output networks can be connected to the input ones). These networks can form arbitrary topologies.</p>
</section></section>
<section>
<section id="feed-forward-networks-1" class="title-slide slide level1 center">
<h1>Feed-forward Networks</h1>

</section>
<section id="feed-forward-networks-2" class="slide level2">
<h2>Feed-forward Networks</h2>
<p>In this type of networks, it is common to organize the neurons in <strong>layers</strong>.</p>
<p>In a layered feed-forward network:</p>
<ul>
<li class="fragment">There are no connections between neurons of the same layer</li>
<li class="fragment">There are no connections with neurons of preceding layers</li>
<li class="fragment">It is fairly simple to make the computations because there are no loops</li>
<li class="fragment">These networks can implement simple reactive agents</li>
</ul>
</section>
<section id="feed-forward-networks-3" class="slide level2 smaller">
<h2>Feed-forward Networks</h2>
<p>Network with two layers:</p>

<img data-src="FeedForwardTwoLayers.png" class="quarto-figure quarto-figure-center r-stretch" style="width:100.0%"><ul>
<li class="fragment">The first layer is composed by units <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span>, which receive signals from the environment</li>
<li class="fragment">On the right side, we have the output units (only one, in this case)</li>
<li class="fragment">All layers, except the output layer, are called <strong>hidden units</strong></li>
</ul>
</section>
<section id="feed-forward-networks-4" class="slide level2">
<h2>Feed-forward Networks</h2>
<p>Some networks don’t have hidden layers. This simplifies the learning problem, but it implies, as already seen, strong limitations on what can be represented by the network.</p>
<p><strong>Important results:</strong></p>
<ul>
<li class="fragment">With a sufficiently large hidden layer, it is possible to represent any <strong>continuous function</strong></li>
<li class="fragment">With two hidden layers, <strong>discontinuous functions</strong> can be represented</li>
</ul>
</section></section>
<section>
<section id="hopfield-networks" class="title-slide slide level1 center">
<h1>Hopfield Networks</h1>

</section>
<section id="overview" class="slide level2">
<h2>Overview</h2>
<p>Hopfield networks are an example of recurrent networks that have been extensively studied.</p>
<p>Characteristics:</p>
<ul>
<li class="fragment">All the network’s units are both input and output neurons</li>
<li class="fragment">The connections are <strong>bidirectional</strong></li>
<li class="fragment">The sign function is used as activation function</li>
</ul>
<!-- IMAGE: Hopfield network diagram showing bidirectional connections -->
</section>
<section id="associative-memory" class="slide level2">
<h2>Associative Memory</h2>
<p>This type of network works as an <strong>associative memory</strong>.</p>
<ul>
<li class="fragment">It is able to memorize <strong>0.138n</strong> examples, where <span class="math inline">\(n\)</span> is the number of neurons</li>
<li class="fragment">For example, if the training set consists of a set of photos and if a small part of one of these photos is later presented to the network, the activation levels of the network should reproduce the photo to which the fragment presented belongs</li>
</ul>
</section></section>
<section>
<section id="network-structure-selection" class="title-slide slide level1 center">
<h1>Network Structure Selection</h1>

</section>
<section id="the-challenge" class="slide level2">
<h2>The Challenge</h2>
<p>One of the problems posed to who wants to build a neural network is the choice of its structure.</p>
<div class="callout callout-warning callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Trade-offs</strong></p>
</div>
<div class="callout-content">
<ul>
<li class="fragment">If the network is <strong>too small</strong> may not be able to represent/model the desired function</li>
<li class="fragment">If it is <strong>too big</strong>, it can be able to represent exactly the training examples but be unable to generalize beyond those examples (we call <strong>overfitting</strong> to this problem)</li>
</ul>
</div>
</div>
</div>
<p>The truth is that there is no method for choosing what is exactly the best structure for a neural network.</p>
</section></section>
<section>
<section id="neural-networks-training" class="title-slide slide level1 center">
<h1>Neural Networks Training</h1>

</section>
<section id="configuration-methods" class="slide level2">
<h2>Configuration Methods</h2>
<p>A neural network must be configured so that a set of inputs produce the desired output.</p>
<p>There are basically two methods to set the network connections’ weights:</p>
<ol type="1">
<li class="fragment">Using <strong>a priori knowledge</strong> (rarely used)</li>
<li class="fragment"><strong>Training</strong> the network using some learning method</li>
</ol>
</section>
<section id="types-of-learning" class="slide level2">
<h2>Types of Learning</h2>
<ul>
<li class="fragment"><strong>Supervised learning</strong></li>
<li class="fragment"><strong>Unsupervised learning</strong></li>
<li class="fragment"><strong>Reinforcement learning</strong></li>
</ul>
<p>These learning paradigms involve the actualization of weights of the connections between neurons, according to some <strong>learning rule</strong>.</p>
</section></section>
<section>
<section id="learning-rules" class="title-slide slide level1 center">
<h1>Learning Rules</h1>
<p>Virtually all learning rules can be considered as variants of the <strong>Hebb’s rule</strong> suggested by Hebb in <em>Organization of Behaviour</em> (1949).</p>
</section>
<section id="hebbs-rule" class="slide level2">
<h2>Hebb’s Rule</h2>
<p><strong>Idea</strong>: If two units <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are simultaneously active, then the connection between them should be strengthened.</p>
<!-- IMAGE: Simple diagram showing units i and j connected by weight wij -->
<p>If unit <span class="math inline">\(i\)</span> receives a value from unit <span class="math inline">\(j\)</span>, then Hebb’s rule prescribes the modification of <span class="math inline">\(w_{ij}\)</span> according to:</p>
<p><span class="math display">\[\Delta w_{ij} = \gamma a_i a_j\]</span></p>
<p>Where:</p>
<ul>
<li class="fragment"><span class="math inline">\(\gamma\)</span> is a positive proportionality constant that represents the <strong>learning rate</strong></li>
<li class="fragment"><span class="math inline">\(a_i\)</span> and <span class="math inline">\(a_j\)</span> represent the output values of units <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, respectively</li>
</ul>
</section>
<section id="delta-rule-widrow-hoff-1960" class="slide level2">
<h2>Delta Rule (Widrow-Hoff 1960)</h2>
<p>Delta rule is a variant of the Hebb’s rule: instead of the activation of unit <span class="math inline">\(i\)</span>, it introduces the difference between the current activation value of unit <span class="math inline">\(i\)</span>, <span class="math inline">\(a_i\)</span>, and the desired activation value, <span class="math inline">\(d_i\)</span>, to update the weights:</p>
<!-- IMAGE: Simple diagram showing units i and j connected by weight wij -->
<p><span class="math display">\[\Delta w_{ij} = \gamma (d_i - a_i) a_j\]</span></p>
</section></section>
<section>
<section id="perceptron" class="title-slide slide level1 center">
<h1>Perceptron</h1>

</section>
<section id="definition" class="slide level2">
<h2>Definition</h2>
<p>We will now study how the training is done in one layer networks using, as example, a <strong>perceptron network</strong>.</p>
<p>A perceptron network is a feedforward network:</p>
<ul>
<li class="fragment">Each neuron (perceptron), as seen before, computes the inputs’ weighted sum and outputs a value of 1 if the sum is larger than <span class="math inline">\(\theta\)</span> and -1, otherwise (the signal activation function is, therefore, used)</li>
<li class="fragment">The goal of a perceptron is the learning of some transformation <span class="math inline">\(T\)</span>, using examples, each composed by the entry <span class="math inline">\(\mathbf{x}\)</span> and by the corresponding desired output <span class="math inline">\(t = T(\mathbf{x})\)</span></li>
</ul>
<p><span class="math display">\[T: \{-1, 1\}^n \to \{-1, 1\}^m\]</span></p>
</section>
<section id="scheme-of-a-perceptron-network" class="slide level2">
<h2>Scheme of a Perceptron Network</h2>
<!-- IMAGE: Perceptron network with inputs x1, x2, x3, x4 and outputs a1, a2, a3 -->
<p>Note that the units are <strong>independent</strong> from each other. This means that we can limit our study to just one perceptron.</p>
</section></section>
<section>
<section id="perceptron-training-algorithm" class="title-slide slide level1 center">
<h1>Perceptron Training Algorithm</h1>

</section>
<section id="algorithm" class="slide level2">
<h2>Algorithm</h2>
<ol type="1">
<li class="fragment">Initialize the weights randomly</li>
<li class="fragment"><strong>Do</strong></li>
<li class="fragment">&nbsp;&nbsp;&nbsp;&nbsp;<strong>For each</strong> training example <span class="math inline">\(\mathbf{x}\)</span> from the train set <strong>do</strong></li>
<li class="fragment">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Compute the output <span class="math inline">\(a\)</span> of the network for input <span class="math inline">\(\mathbf{x}\)</span></li>
<li class="fragment">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>If</strong> <span class="math inline">\(a \neq T(\mathbf{x})\)</span> (the perceptron answers incorrectly), modify the connections weights (including <span class="math inline">\(\theta\)</span>) as follows:</li>
</ol>
<p><span class="math display">\[\Delta w_i = \gamma (t - a) x_i\]</span></p>
<p>where <span class="math inline">\(\gamma\)</span> represents the learning rate and should be <span class="math inline">\(0 &lt; \gamma \leq 1\)</span></p>
<ol start="6" type="1">
<li class="fragment"><strong>Until</strong> <span class="math inline">\(a = T(\mathbf{x})\)</span> for all training examples</li>
</ol>
</section>
<section id="relationship-to-hebbs-rule" class="slide level2">
<h2>Relationship to Hebb’s Rule</h2>
<p>This procedure is very similar to the Hebb’s rule. The only difference is that, if the network answer is correct, there is no weights update.</p>
<p><span class="math display">\[\text{Error} = \text{desired output} - \text{current output}\]</span></p>
</section></section>
<section>
<section id="perceptron-training-example" class="title-slide slide level1 center">
<h1>Perceptron Training Example</h1>

</section>
<section id="problem-setup" class="slide level2">
<h2>Problem Setup</h2>
<p>We want that the perceptron learns the logic AND function using a learning rate with value 1.</p>
<p>Let us consider that the perceptron is initialized in the following way:</p>
<!-- IMAGE: Perceptron with inputs x1, x2, bias -1, and weights w1=0, w2=0, θ=0 -->
<p>Initial weights: <span class="math inline">\(w_1 = 0\)</span>, <span class="math inline">\(w_2 = 0\)</span>, <span class="math inline">\(\theta = 0\)</span></p>
<p>The training examples consist of vectors <span class="math inline">\([x_1, x_2]\)</span> with values <span class="math inline">\([-1, -1]\)</span>, <span class="math inline">\([-1, 1]\)</span>, <span class="math inline">\([1, -1]\)</span> and <span class="math inline">\([1, 1]\)</span> (corresponding to the combinations that the AND function inputs can have) associated to the corresponding desired output: <span class="math inline">\(-1, -1, -1, 1\)</span>, respectively.</p>
</section>
<section id="st-presentation-of-training-set" class="slide level2">
<h2>1st Presentation of Training Set</h2>
<p><strong>Vector [-1, -1]:</strong></p>
<p><span class="math display">\[a = \text{signal}(w_1 \cdot x_1 + w_2 \cdot x_2 + 0 \cdot (-1)) = \text{signal}(0 + 0 + 0) = \text{signal}(0) = -1\]</span></p>
<p>which corresponds to the output of <span class="math inline">\(-1\)</span> AND <span class="math inline">\(-1\)</span>. Therefore, nothing is done.</p>
<p><strong>Vector [-1, 1]:</strong></p>
<p><span class="math display">\[a = \text{signal}(0 \cdot (-1) + 0 \cdot 1 + 0 \cdot (-1)) = \text{signal}(0) = -1\]</span></p>
<p>which corresponds to the output of <span class="math inline">\(-1\)</span> AND <span class="math inline">\(1\)</span>. Therefore, nothing is done.</p>
</section>
<section id="st-presentation-continued" class="slide level2">
<h2>1st Presentation (continued)</h2>
<p><strong>Vector [1, -1]:</strong></p>
<p><span class="math display">\[a = \text{signal}(0 \cdot 1 + 0 \cdot (-1) + 0 \cdot (-1)) = \text{signal}(0) = -1\]</span></p>
<p>which corresponds to the output of <span class="math inline">\(1\)</span> AND <span class="math inline">\(-1\)</span>. Therefore, nothing is done.</p>
<p><strong>Vector [1, 1]:</strong></p>
<p><span class="math display">\[a = \text{signal}(0 \cdot 1 + 0 \cdot 1 + 0 \cdot (-1)) = \text{signal}(0) = -1\]</span></p>
<p>which doesn’t correspond to the output of <span class="math inline">\(1\)</span> AND <span class="math inline">\(1\)</span>. Let, therefore, update the weights:</p>
<ul>
<li class="fragment"><span class="math inline">\(w_1(\text{new}) = w_1(\text{current}) + \gamma \cdot (t - a) \cdot x = 0 + 1 \cdot 2 \cdot 1 = 2\)</span></li>
<li class="fragment"><span class="math inline">\(w_2(\text{new}) = w_2(\text{current}) + \gamma \cdot (t - a) \cdot x = 0 + 1 \cdot 2 \cdot 1 = 2\)</span></li>
<li class="fragment"><span class="math inline">\(\theta(\text{new}) = \theta(\text{current}) + \gamma \cdot (t - a) \cdot x = 0 + 1 \cdot 2 \cdot (-1) = -2\)</span></li>
</ul>
</section>
<section id="nd-presentation-of-training-set" class="slide level2">
<h2>2nd Presentation of Training Set</h2>
<p><strong>Current weights:</strong> <span class="math inline">\([2, 2, -2]\)</span></p>
<p><strong>Vector [-1, -1]:</strong></p>
<p><span class="math display">\[a = \text{signal}(2 \cdot (-1) + 2 \cdot (-1) + (-2) \cdot (-1)) = \text{signal}(-2) = -1\]</span></p>
<p>which corresponds to the output of <span class="math inline">\(-1\)</span> AND <span class="math inline">\(-1\)</span>. Therefore, nothing is done.</p>
<p><strong>Vector [-1, 1]:</strong></p>
<p><span class="math display">\[a = \text{signal}(2 \cdot (-1) + 2 \cdot 1 + (-2) \cdot (-1)) = \text{signal}(2) = 1\]</span></p>
<p>which doesn’t correspond to the output of <span class="math inline">\(-1\)</span> AND <span class="math inline">\(1\)</span>. Let us, therefore update the weights:</p>
<ul>
<li class="fragment"><span class="math inline">\(w_1(\text{new}) = w_1(\text{current}) + \gamma \cdot (t - a) \cdot x = 2 + 1 \cdot (-2) \cdot (-1) = 4\)</span></li>
<li class="fragment"><span class="math inline">\(w_2(\text{new}) = w_2(\text{current}) + \gamma \cdot (t - a) \cdot x = 2 + 1 \cdot (-2) \cdot 1 = 0\)</span></li>
<li class="fragment"><span class="math inline">\(\theta(\text{new}) = \theta(\text{current}) + \gamma \cdot (t - a) \cdot x = -2 + 1 \cdot (-2) \cdot (-1) = 0\)</span></li>
</ul>
</section>
<section id="nd-presentation-continued" class="slide level2">
<h2>2nd Presentation (continued)</h2>
<p><strong>Current weights:</strong> <span class="math inline">\([4, 0, 0]\)</span></p>
<p><strong>Vector [1, -1]:</strong></p>
<p><span class="math display">\[a = \text{signal}(4 \cdot 1 + 0 \cdot (-1) + 0 \cdot (-1)) = \text{signal}(4) = 1\]</span></p>
<p>which doesn’t correspond to the output of <span class="math inline">\(1\)</span> AND <span class="math inline">\(-1\)</span>. Let us update the weights:</p>
<ul>
<li class="fragment"><span class="math inline">\(w_1(\text{new}) = w_1(\text{current}) + \gamma \cdot (t - a) \cdot x = 4 + 1 \cdot (-2) \cdot 1 = 2\)</span></li>
<li class="fragment"><span class="math inline">\(w_2(\text{new}) = w_2(\text{current}) + \gamma \cdot (t - a) \cdot x = 0 + 1 \cdot (-2) \cdot (-1) = 2\)</span></li>
<li class="fragment"><span class="math inline">\(\theta(\text{new}) = \theta(\text{current}) + \gamma \cdot (t - a) \cdot x = 0 + 1 \cdot (-2) \cdot (-1) = 2\)</span></li>
</ul>
<p><strong>Current weights:</strong> <span class="math inline">\([2, 2, 2]\)</span></p>
<p><strong>Vector [1, 1]:</strong></p>
<p><span class="math display">\[a = \text{signal}(2 \cdot 1 + 2 \cdot 1 + 2 \cdot (-1)) = \text{signal}(2) = 1\]</span></p>
<p>which corresponds to the output of <span class="math inline">\(1\)</span> AND <span class="math inline">\(1\)</span>. Therefore, nothing is done.</p>
</section>
<section id="rd-presentation-of-training-set" class="slide level2">
<h2>3rd Presentation of Training Set</h2>
<p><strong>Current weights:</strong> <span class="math inline">\([2, 2, 2]\)</span></p>
<p><strong>Vector [-1, -1]:</strong></p>
<p><span class="math display">\[a = \text{signal}(2 \cdot (-1) + 2 \cdot (-1) + 2 \cdot (-1)) = \text{signal}(-6) = -1\]</span></p>
<p>which corresponds to the output of <span class="math inline">\(-1\)</span> AND <span class="math inline">\(-1\)</span>. Therefore, nothing is done.</p>
<p><strong>Vector [-1, 1]:</strong></p>
<p><span class="math display">\[a = \text{signal}(2 \cdot (-1) + 2 \cdot 1 + 2 \cdot (-1)) = \text{signal}(-2) = -1\]</span></p>
<p>which corresponds to the output of <span class="math inline">\(-1\)</span> AND <span class="math inline">\(1\)</span>. Therefore, nothing is done.</p>
</section>
<section id="rd-presentation-continued" class="slide level2">
<h2>3rd Presentation (continued)</h2>
<p><strong>Vector [1, -1]:</strong></p>
<p><span class="math display">\[a = \text{signal}(2 \cdot 1 + 2 \cdot (-1) + 2 \cdot (-1)) = \text{signal}(-2) = -1\]</span></p>
<p>which corresponds to the output of <span class="math inline">\(1\)</span> AND <span class="math inline">\(-1\)</span>. Therefore, nothing is done.</p>
<p><strong>Vector [1, 1]:</strong></p>
<p><span class="math display">\[a = \text{signal}(2 \cdot 1 + 2 \cdot 1 + 2 \cdot (-1)) = \text{signal}(2) = 1\]</span></p>
<p>which corresponds to the output of <span class="math inline">\(1\)</span> AND <span class="math inline">\(1\)</span>. Therefore, nothing is done.</p>
</section>
<section id="final-result" class="slide level2">
<h2>Final Result</h2>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Learning Complete</strong></p>
</div>
<div class="callout-content">
<p>The learning process is finished. The set of weights that allows the perceptron to answer correctly is:</p>
<ul>
<li class="fragment"><span class="math inline">\(w_1 = 2\)</span></li>
<li class="fragment"><span class="math inline">\(w_2 = 2\)</span></li>
<li class="fragment"><span class="math inline">\(\theta = 2\)</span></li>
</ul>
</div>
</div>
</div>
</section></section>
<section>
<section id="the-delta-rule-or-lms" class="title-slide slide level1 center">
<h1>The Delta Rule or LMS</h1>

</section>
<section id="overview-1" class="slide level2">
<h2>Overview</h2>
<p>The Delta rule, also known as <strong>LMS (Least Mean Square)</strong> is a generalization of the perceptron’s training algorithm, extended to be able to deal with continuous inputs and outputs.</p>
<p>According to this rule, the weights update is done in the following way:</p>
<p><span class="math display">\[\Delta w_{jp} = \gamma \delta_p x_{jp}\]</span></p>
<p>where <span class="math inline">\(\delta_p = d_p - a_p\)</span> is the difference between the desired and the current output for the input pattern <span class="math inline">\(p\)</span>.</p>
</section>
<section id="lms-with-linear-units" class="slide level2">
<h2>LMS with Linear Units</h2>
<p>The LMS procedure has been applied more often with purely linear output units.</p>
<p>For a network of this type (linear output) with just one output unit, the output is given by:</p>
<p><span class="math display">\[a = \sum_j w_j x_j - \theta\]</span></p>
<p>The error function, as the name indicates, is the quadratic sum of the weights, that is, the total error, <span class="math inline">\(E\)</span>, is defined by:</p>
<p><span class="math display">\[E = \sum_p E_p = \sum_p \frac{1}{2}(d_p - a_p)^2\]</span></p>
</section>
<section id="gradient-descent-method" class="slide level2">
<h2>Gradient Descent Method</h2>
<p>The LMS procedure finds the weights values which minimize the function error, through a method called <strong>gradient descent</strong>.</p>
<p>The idea consists in doing changes to the weights proportional to the negative derivative of the error:</p>
<p><span class="math display">\[\Delta w_j = -\gamma \frac{\partial E}{\partial w_j}\]</span></p>
<p><span class="math display">\[\frac{\partial E}{\partial w_j} = \sum_p \frac{\partial E_p}{\partial w_j} = \sum_p \frac{\partial E_p}{\partial a_p} \frac{\partial a_p}{\partial w_j}\]</span></p>
<!-- IMAGE: Error surface diagram showing gradient descent -->
</section>
<section id="derivation" class="slide level2">
<h2>Derivation</h2>
<p>Given that the units are linear, we have:</p>
<p><span class="math display">\[\frac{\partial a_p}{\partial w_j} = x_j\]</span></p>
<p>and</p>
<p><span class="math display">\[\frac{\partial E_p}{\partial a_p} = -(d_p - a_p)\]</span></p>
<p>which leads to:</p>
<p><span class="math display">\[\Delta w_{jp} = \gamma \delta_p x_j\]</span></p>
<p>where <span class="math inline">\(\delta_p = d_p - a_p\)</span> is the difference between the desired output and the current output for the training example <span class="math inline">\(p\)</span>.</p>
</section></section>
<section>
<section id="back-propagation" class="title-slide slide level1 center">
<h1>Back-propagation</h1>

</section>
<section id="historical-context" class="slide level2">
<h2>Historical Context</h2>
<p>Minsky and Papert showed in 1969 that a feed-forward network with two layers could solve many of the restrictions found until then, but presented no solution for the problem of adjusting the weights of the hidden layers.</p>
</section>
<section id="the-solution" class="slide level2">
<h2>The Solution</h2>
<p>In 1986 Rumelhart, Hinton and Williams presented a solution for this problem.</p>
<p>The central idea of this solution is that the <strong>errors for the hidden layers units are computed backpropagating the errors of the output layer units</strong>.</p>
<p>The algorithm can also be considered a generalization of the Delta rule for non-linear activation functions and can be applied to networks with any number of layers.</p>
</section></section>
<section>
<section id="back-propagation-algorithm" class="title-slide slide level1 center">
<h1>Back-propagation Algorithm</h1>

</section>
<section id="the-algorithm" class="slide level2">
<h2>The Algorithm</h2>
<ol type="1">
<li class="fragment">Initialize all the weights with low random values</li>
<li class="fragment"><strong>While</strong> stop condition not satisfied <strong>do</strong>
<ul>
<li class="fragment"><strong>For each</strong> training example <strong>do</strong>
<ul>
<li class="fragment">Compute the output <span class="math inline">\(o\)</span> of the network for the training example<br>
<em>// propagates the inputs through the network</em></li>
<li class="fragment"><strong>For each</strong> output unit <span class="math inline">\(k\)</span> <strong>do</strong><br>
<em>// computes and propagates errors through the network</em> <span class="math display">\[\delta_k = f'(in_k)(d_k - a_k)\]</span></li>
<li class="fragment"><strong>For each</strong> hidden unit <span class="math inline">\(h\)</span> <strong>do</strong> <span class="math display">\[\delta_h = f'(in_h) \sum_k w_{h,k} \delta_k\]</span></li>
<li class="fragment">Update each weight of the network: <span class="math display">\[w_{i,j} = w_{i,j} + \gamma \delta_j a_i\]</span></li>
</ul></li>
</ul></li>
</ol>
</section>
<section id="sigmoid-activation-function" class="slide level2">
<h2>Sigmoid Activation Function</h2>
<p>Let us consider the sigmoid activation function:</p>
<p><span class="math display">\[f(x) = \frac{1}{1 + e^{-x}}\]</span></p>
<p>whose derivative is:</p>
<p><span class="math display">\[f'(x) = f(x) \cdot (1 - f(x))\]</span></p>
</section>
<section id="error-computation-for-output-units" class="slide level2">
<h2>Error Computation for Output Units</h2>
<p>So, the error for an output unit <span class="math inline">\(k\)</span> is given by:</p>
<p><span class="math display">\[\delta_k = f'(in_k) \cdot (d_k - a_k)\]</span></p>
<p><span class="math display">\[\delta_k = f(in_k) \cdot (1 - f(in_k)) \cdot (d_k - a_k)\]</span></p>
<p><span class="math display">\[\delta_k = a_k \cdot (1 - a_k) \cdot (d_k - a_k)\]</span></p>
<p>That is, for the output units, the error is:</p>
<p><span class="math display">\[\boxed{\delta_k = a_k \cdot (1 - a_k) \cdot (d_k - a_k)}\]</span></p>
<p>Components:</p>
<ul>
<li class="fragment">Desired output: <span class="math inline">\(d_k\)</span></li>
<li class="fragment">Current output: <span class="math inline">\(a_k\)</span></li>
</ul>
</section>
<section id="error-computation-for-hidden-units" class="slide level2">
<h2>Error Computation for Hidden Units</h2>
<p>For the hidden layers units <span class="math inline">\(h\)</span>, the error is given by:</p>
<p><span class="math display">\[\boxed{\delta_h = a_h \cdot (1 - a_h) \cdot \sum_k w_{h,k} \delta_k}\]</span></p>
<p>Components:</p>
<ul>
<li class="fragment">Current output of unit <span class="math inline">\(h\)</span>: <span class="math inline">\(a_h\)</span></li>
<li class="fragment">Weight of the connection to destination unit <span class="math inline">\(k\)</span>: <span class="math inline">\(w_{h,k}\)</span></li>
<li class="fragment">Error for the destination unit <span class="math inline">\(k\)</span>: <span class="math inline">\(\delta_k\)</span></li>
<li class="fragment">Forward connections: <span class="math inline">\(\sum_k\)</span></li>
</ul>
</section>
<section id="weight-update" class="slide level2">
<h2>Weight Update</h2>
<p>And the new weights:</p>
<p><span class="math display">\[w_{i,j} = w_{i,j} + \gamma \delta_j a_i\]</span></p>
<p>Components:</p>
<ul>
<li class="fragment">Current value: <span class="math inline">\(w_{i,j}\)</span></li>
<li class="fragment">Error for the destination unit: <span class="math inline">\(\delta_j\)</span></li>
<li class="fragment">Output value for the origin unit: <span class="math inline">\(a_i\)</span></li>
</ul>
</section></section>
<section>
<section id="back-propagation-exercise" class="title-slide slide level1 center">
<h1>Back-propagation Exercise</h1>

</section>
<section id="problem" class="slide level2">
<h2>Problem</h2>
<p>Given the below network and the following training example <span class="math inline">\(\langle [0, 1], [1, 1] \rangle\)</span> (<span class="math inline">\(\langle\)</span>input, desired output<span class="math inline">\(\rangle\)</span>), update the weights using Back-propagation.</p>
<!-- IMAGE: Neural network with h1, h2 hidden units and o1, o2 output units with specified weights -->
<p>Network structure:</p>
<ul>
<li class="fragment">Inputs: <span class="math inline">\([0, 1]\)</span></li>
<li class="fragment">Hidden units: <span class="math inline">\(h_1\)</span>, <span class="math inline">\(h_2\)</span></li>
<li class="fragment">Output units: <span class="math inline">\(o_1\)</span>, <span class="math inline">\(o_2\)</span></li>
<li class="fragment">Weights shown in diagram</li>
<li class="fragment">Activation function: <strong>sigmoid</strong></li>
<li class="fragment">Learning rate: <span class="math inline">\(\gamma = 0.25\)</span></li>
</ul>
</section>
<section id="solution-forward-pass" class="slide level2">
<h2>Solution: Forward Pass</h2>
<p>First, we need to compute the network output for input <span class="math inline">\([0, 1]\)</span>:</p>
<p><strong>Output of <span class="math inline">\(h_1\)</span>:</strong></p>
<p><span class="math display">\[f(0 \cdot (-0.1) + 1 \cdot 0.3) = f(0.3) = \frac{1}{1+e^{-0.3}} = 0.5744\]</span></p>
<p><strong>Output of <span class="math inline">\(h_2\)</span>:</strong></p>
<p><span class="math display">\[f(0 \cdot 0.2 + 1 \cdot (-0.2)) = f(-0.2) = \frac{1}{1+e^{0.2}} = 0.4502\]</span></p>
<p><strong>Output of <span class="math inline">\(o_1\)</span>:</strong></p>
<p><span class="math display">\[f(0.5744 \cdot 0.2 + 0.4502 \cdot (-0.5)) = f(-0.1102) = \frac{1}{1+e^{0.1102}} = 0.4725\]</span></p>
<p><strong>Output of <span class="math inline">\(o_2\)</span>:</strong></p>
<p><span class="math display">\[f(0.5744 \cdot 0.2 + 0.4502 \cdot (-0.1)) = f(-0.0698) = \frac{1}{1+e^{-0.0698}} = 0.5175\]</span></p>
</section>
<section id="solution-delta-errors" class="slide level2">
<h2>Solution: Delta Errors</h2>
<p><strong>Delta errors:</strong></p>
<p><span class="math display">\[\delta_{o_1} = (1 - 0.4725) \cdot 0.4725 \cdot (1 - 0.4725) = 0.1315\]</span></p>
<p><span class="math display">\[\delta_{o_2} = (1 - 0.5175) \cdot 0.5175 \cdot (1 - 0.5175) = 0.1205\]</span></p>
<p><span class="math display">\[\delta_{h_1} = 0.5744 \cdot (1 - 0.5744) \cdot (0.2 \cdot 0.1315 + 0.2 \cdot 0.1205) = 0.0123\]</span></p>
<p><span class="math display">\[\delta_{h_2} = 0.4502 \cdot (1 - 0.4502) \cdot (-0.5 \cdot 0.1315 + (-0.1) \cdot 0.1205) = -0.0193\]</span></p>
</section>
<section id="solution-weight-updates" class="slide level2">
<h2>Solution: Weight Updates</h2>
<p>Using <span class="math inline">\(\gamma = 0.25\)</span>:</p>
<p><span class="math display">\[w_{h_1x_1} = -0.1 + 0.25 \cdot 0.0123 \cdot 0 = -0.1\]</span></p>
<p><span class="math display">\[w_{h_1x_2} = 0.3 + 0.25 \cdot 0.0123 \cdot 1 = 0.3031\]</span></p>
<p><span class="math display">\[w_{h_2x_1} = 0.2 + 0.25 \cdot (-0.01) \cdot 0 = 0.2\]</span></p>
<p><span class="math display">\[w_{h_2x_2} = -0.2 + 0.25 \cdot (-0.0193) \cdot 1 = -0.2048\]</span></p>
<p><span class="math display">\[w_{o_1h_1} = 0.2 + 0.25 \cdot 0.1315 \cdot 0.5744 = 0.2189\]</span></p>
<p><span class="math display">\[w_{o_1h_2} = -0.5 + 0.25 \cdot 0.1315 \cdot 0.4502 = -0.4852\]</span></p>
<p><span class="math display">\[w_{o_2h_1} = 0.2 + 0.25 \cdot 0.1205 \cdot 0.5744 = 0.2173\]</span></p>
<p><span class="math display">\[w_{o_2h_2} = -0.1 + 0.25 \cdot 0.1205 \cdot 0.4502 = -0.0864\]</span></p>
</section></section>
<section>
<section id="back-propagation-problems" class="title-slide slide level1 center">
<h1>Back-propagation Problems</h1>

</section>
<section id="long-training-periods" class="slide level2">
<h2>Long Training Periods</h2>
<p>Long training periods can be due to an inadequate learning rate:</p>
<div class="callout callout-warning callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Learning Rate Issues</strong></p>
</div>
<div class="callout-content">
<ul>
<li class="fragment">If we use a learning rate <strong>too small</strong>, the algorithm may take too long to converge</li>
<li class="fragment">If we use a learning rate <strong>too high</strong>, the weights can oscillate, making it difficult for convergence to good solutions (weights close to the minimum error value)</li>
</ul>
</div>
</div>
</div>
</section>
<section id="unit-saturation" class="slide level2">
<h2>Unit Saturation</h2>
<p>As the training process advances, the weights can reach very high values.</p>
<p>This can lead to:</p>
<ul>
<li class="fragment">Very high weighted inputs sums</li>
<li class="fragment">Due to the usage of the sigmoid activation function, the units will have activation values <span class="math inline">\(a\)</span> very close to zero</li>
<li class="fragment">In these circumstances, the variation of weights, which is proportional to <span class="math inline">\(a \cdot (1 - a)\)</span>, will be close to zero and the network doesn’t learn</li>
</ul>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Solution</strong></p>
</div>
<div class="callout-content">
<p>A good way of avoiding this problem is to <strong>normalize the entries in the [0, 1] interval</strong>.</p>
</div>
</div>
</div>
</section>
<section id="local-minima" class="slide level2">
<h2>Local Minima</h2>
<p>The error surface of a complex network has plenty of hills and valleys, and the network can be stuck in a <strong>local minimum</strong>, even if there is a nearby global minimum.</p>
<!-- IMAGE: Error surface showing local and global minima -->
</section></section>
<section>
<section id="improving-bps-convergence" class="title-slide slide level1 center">
<h1>Improving BP’s Convergence</h1>

</section>
<section id="techniques" class="slide level2">
<h2>Techniques</h2>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important Note</strong></p>
</div>
<div class="callout-content">
<p>These techniques do not grant a faster convergence and/or to a global minimum (especially the first two). However, they often lead to better results.</p>
</div>
</div>
</div>
<ol type="1">
<li class="fragment">Change the presentation order of the training examples</li>
<li class="fragment">Start with a high learning rate and diminish it gradually as the training process progresses</li>
<li class="fragment"><strong>Momentum</strong> (see next section)</li>
<li class="fragment">Others…</li>
</ol>
</section>
<section id="momentum" class="slide level2">
<h2>Momentum</h2>
<p><strong>Goal</strong>: To accelerate the learning process amortizing oscillations and facilitating the convergence to a global minimum of the error function.</p>
<p><strong>How</strong>: The variation of weights depends also on previous variations:</p>
<p><span class="math inline">\(w_{i,j}(t) = \gamma \delta_j a_i + \alpha \Delta w_{i,j}(t-1)\)</span></p>
<p>Where:</p>
<ul>
<li class="fragment"><span class="math inline">\(\gamma\)</span> is the learning rate</li>
<li class="fragment"><span class="math inline">\(\alpha\)</span> is the momentum coefficient</li>
<li class="fragment"><span class="math inline">\(\Delta w_{i,j}(t-1)\)</span> is the previous weight change</li>
</ul>
</section>
<section id="momentum-effects" class="slide level2">
<h2>Momentum Effects</h2>
<!-- IMAGE: Two error surface diagrams comparing without momentum vs with momentum -->
<p><strong>Without momentum:</strong></p>
<ul>
<li class="fragment">It isn’t able to escape from the local minimum due to oscillations</li>
</ul>
<p><strong>With momentum:</strong></p>
<ul>
<li class="fragment">It can escape from the local minimum: the weights tend to be modified in the direction of previous modifications</li>
<li class="fragment">Successive updates in the same direction tend to increase the magnitude of the modifications, leading to a faster convergence</li>
</ul>
</section></section>
<section>
<section id="back-propagation-applications" class="title-slide slide level1 center">
<h1>Back-propagation Applications</h1>

</section>
<section id="general-applications" class="slide level2">
<h2>General Applications</h2>
<p>In general: learning of some function by the way of training examples. The network should be able to generalize.</p>
<p><strong>Example</strong>: Controlling the arm of a Robot</p>
</section>
<section id="historical-example" class="slide level2">
<h2>Historical Example</h2>
<p><strong>NETTALK (1987)</strong> was a system able to convert written English to intelligible speech.</p>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Search for “sejnowski nettalk” on YouTube to see it in action.</p>
</div>
</div>
</div>
</section>
<section id="other-applications" class="slide level2">
<h2>Other Applications</h2>
<ul>
<li class="fragment">Signals classification (e.g., sonar)</li>
<li class="fragment">Pattern recognition</li>
<li class="fragment">Function approximation</li>
</ul>
</section></section>
<section>
<section id="training-neural-networks" class="title-slide slide level1 center">
<h1>Training Neural Networks</h1>

</section>
<section id="the-training-set" class="slide level2">
<h2>The Training Set</h2>
<p>Usually, it is not possible to train a neural network with all the examples of the universe of interest because:</p>
<ul>
<li class="fragment">It is often too large or infinite (e.g., the inputs/outputs are real numbers)</li>
<li class="fragment">We just have access to a subset of the universe</li>
</ul>
<p>In order to train the network, a subset of all the universe of possibilities is used. This subset, called the <strong>training set</strong>, is hoped to be representative of the function that the neural network is supposed to learn.</p>
</section>
<section id="the-test-set" class="slide level2">
<h2>The Test Set</h2>
<p>After the training process, the neural network is tested with the so-called <strong>test set</strong> (with no common examples with the training set).</p>
<p>This is done in order to verify if the network is able to <strong>generalize</strong> beyond the examples used in the training process.</p>
<p><strong>Process:</strong></p>
<ul>
<li class="fragment">If the neural network has a good performance with the test set, the training process is finished</li>
<li class="fragment">If not, the training process must be repeated after something has been changed (the training set, the learning rate, the network architecture, etc.)</li>
</ul>
</section></section>
<section>
<section id="neural-networks-examples" class="title-slide slide level1 center">
<h1>Neural Networks Examples</h1>

</section>
<section id="application-domains" class="slide level2">
<h2>Application Domains</h2>
<ul>
<li class="fragment"><strong>Voice/image recognition</strong> (faces detection/identification, sentiment analysis)</li>
<li class="fragment"><strong>Translation</strong></li>
<li class="fragment"><strong>Industrial control</strong></li>
<li class="fragment"><strong>Financial analysis</strong></li>
<li class="fragment"><strong>Medicine</strong></li>
<li class="fragment"><strong>Robotics</strong></li>
<li class="fragment"><strong>Pattern classification</strong></li>
<li class="fragment"><strong>Non-linear control</strong></li>
</ul>
</section></section>
<section>
<section id="final-notes" class="title-slide slide level1 center">
<h1>Final Notes</h1>

</section>
<section id="characteristics" class="slide level2">
<h2>Characteristics</h2>
<p>Neural networks are specially suitable for situations where inputs and outputs have <strong>continuous values</strong>.</p>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Key Properties</strong></p>
</div>
<div class="callout-content">
<ul>
<li class="fragment">They are quite <strong>resistant to noise</strong></li>
<li class="fragment">They work as a <strong>“black box”</strong>: they do not provide any explanation for the output</li>
<li class="fragment">This turns difficult the usage of knowledge to set up the network as, for example, setting the weights</li>
</ul>
</div>
</div>
</div>
</section></section>
<section>
<section id="further-reading" class="title-slide slide level1 center">
<h1>Further Reading</h1>

</section>
<section id="topics-to-explore" class="slide level2">
<h2>Topics to Explore</h2>
<ul>
<li class="fragment"><strong>Cross-validation</strong></li>
<li class="fragment"><strong>Evaluation metrics</strong>:
<ul>
<li class="fragment">Confusion matrix</li>
<li class="fragment">Precision, recall, F-measure</li>
<li class="fragment">Mean absolute error</li>
<li class="fragment">Mean absolute percentage error</li>
<li class="fragment">Root mean squared error</li>
<li class="fragment">Relative absolute error</li>
</ul></li>
<li class="fragment"><strong>Advanced architectures</strong>:
<ul>
<li class="fragment">Kohonen networks (self-organized maps)</li>
<li class="fragment">Deep learning</li>
<li class="fragment">Convolutional neural networks</li>
</ul></li>
<li class="fragment"><strong>Tools and frameworks</strong>:
<ul>
<li class="fragment">Weka</li>
<li class="fragment">TensorFlow</li>
</ul></li>
<li class="fragment">And much more…</li>
</ul>
</section>
<section id="additional-resources" class="slide level2">
<h2>Additional Resources</h2>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Recommended Reading</strong></p>
</div>
<div class="callout-content">
<p>Explore the references mentioned at the beginning of this document for deeper understanding of neural networks concepts and applications.</p>
</div>
</div>
</div>
<h3 id="section"></h3>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>References</strong></p>
</div>
<div class="callout-content">
<ul>
<li class="fragment">Chapter 19 “Artificial Intelligence: A Modern Approach”</li>
<li class="fragment">Chapter 5 “Inteligência Artificial: Fundamentos e Aplicações”</li>
<li class="fragment">“Neural Network Design”, Martin T. Hagan</li>
</ul>
</div>
</div>
</div>

</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<p>AIAG - Genetic Algorithms - Games and Multimedia</p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="05_NeuralNetworks_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="05_NeuralNetworks_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="05_NeuralNetworks_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="05_NeuralNetworks_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="05_NeuralNetworks_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="05_NeuralNetworks_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="05_NeuralNetworks_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="05_NeuralNetworks_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="05_NeuralNetworks_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="05_NeuralNetworks_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="05_NeuralNetworks_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>